{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "\n",
    "from models.utils import set_logger\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-dir', type=str, required=True, help=\"Base data directory.\")\n",
    "\n",
    "DATABASE_IP = 'localhost'\n",
    "DATABASE_PORT = '5432'\n",
    "DATABASE_USERNAME = 'postgres'\n",
    "DATABASE_PASSWORD = 'root'\n",
    "\n",
    "\n",
    "def load_dataset_from_csv(data_dir: str) -> (pd.DataFrame, pd.Series, [str], str):\n",
    "    dataset_path = os.path.join(data_dir, 'dataset_and_target.csv')\n",
    "    features_path = os.path.join(data_dir, 'features.txt')\n",
    "    percentage_features_path = os.path.join(data_dir, 'percentage_features.txt')\n",
    "    target_path = os.path.join(data_dir, 'target.txt')\n",
    "\n",
    "    assert os.path.isfile(dataset_path), f\"No dataset fount at {dataset_path}\"\n",
    "    assert os.path.isfile(features_path), f\"No features found at {features_path}\"\n",
    "    assert os.path.isfile(\n",
    "        percentage_features_path), f\"No percentage features found at {percentage_features_path}\"\n",
    "    assert os.path.isfile(target_path), f\"No target found at {target_path}\"\n",
    "\n",
    "    features = [feature.rstrip() for feature in open(features_path, mode='r').readlines()]\n",
    "    percentage_features = [feature.rstrip() for feature in\n",
    "                           open(percentage_features_path, mode='r').readlines()]\n",
    "    target = open(target_path, mode='r').read()\n",
    "\n",
    "    full_table = pd.read_csv(dataset_path)\n",
    "    return full_table[features], full_table[target], features, percentage_features, target\n",
    "\n",
    "\n",
    "def download_data(query: str, database_name: str, features: [str], target: str) -> (\n",
    "        pd.DataFrame, pd.Series):\n",
    "    # Connect to the database.\n",
    "    db_conn_str = f\"postgresql://{DATABASE_USERNAME}:{DATABASE_PASSWORD}@{DATABASE_IP}:{DATABASE_PORT}/code_classifier\"\n",
    "    db_conn = sqlalchemy.create_engine(db_conn_str)\n",
    "\n",
    "    # Split features and target\n",
    "    full_table = pd.read_sql_query(sql=query, con=db_conn)\n",
    "    return full_table[features], full_table[[target]].iloc[:, 0]\n",
    "\n",
    "\n",
    "def normalize_datatypes(x: pd.DataFrame, y: pd.Series) -> (pd.DataFrame, pd.Series, [str]):\n",
    "    x = pd.get_dummies(x)\n",
    "    x = x.astype('float32')\n",
    "    y = y.apply(lambda value: 0 if value == \"low\" else 1)  # high will be 1 and low will be 0.\n",
    "    y = y.astype('float32')\n",
    "    x = x.fillna(0.0)\n",
    "    columns_names = x.columns.tolist()\n",
    "    return x, y, columns_names\n",
    "\n",
    "\n",
    "def scale_data_to_range_0_1(x: pd.DataFrame, feature_names: [str],\n",
    "                            percentage_feature_names: [str]) -> pd.DataFrame:\n",
    "    for column in feature_names:\n",
    "        if column in percentage_feature_names:\n",
    "            x[column] = x[column] / 100.0\n",
    "        else:\n",
    "            x[column] = x[column] / x[column].max()\n",
    "    x = x.fillna(0.0)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_dataset(database_name: str, data_dir: str) -> None:\n",
    "    # Load paths from data directory.\n",
    "    query_path = os.path.join(data_dir, 'query.sql')\n",
    "    features_path = os.path.join(data_dir, 'features.txt')\n",
    "    target_path = os.path.join(data_dir, 'target.txt')\n",
    "\n",
    "    # Ensure paths exists.\n",
    "    assert os.path.isfile(query_path), f\"No json configuration file found at {query_path}\"\n",
    "    assert os.path.isfile(features_path), f\"No json configuration file found at {features_path}\"\n",
    "    assert os.path.isfile(target_path), f\"No json configuration file found at {target_path}\"\n",
    "\n",
    "    # Download the dataset from the given database.\n",
    "    logging.info(f\"Downloading dataset [{data_dir}]...\")\n",
    "    query = open(query_path, mode='r').read()\n",
    "    features = [feature.rstrip() for feature in open(features_path, mode='r').readlines()]\n",
    "    target = open(target_path, mode='r').read()\n",
    "    x, y = download_data(query=query, database_name=database_name, features=features, target=target)\n",
    "    logging.info(f\"Dataset downloaded. Features shape {x.shape}. Target shape {y.shape}.\")\n",
    "\n",
    "    logging.info(\"Saving dataset to a single csv.\")\n",
    "    x[target] = y\n",
    "    out_file_path = os.path.join(data_dir, 'dataset_and_target.csv')\n",
    "    x.to_csv(out_file_path, index=False)\n",
    "    logging.info(\"Dataset saved.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load the data for the experiment\n",
    "    # args = parser.parse_args()\n",
    "    data_dir = \"data/base/programs\"\n",
    "    # Check for data dir.\n",
    "    assert os.path.isdir(args.data_dir), f\"No data dir for data at {args.data_dir}.\"\n",
    "\n",
    "    # Create and check that base and unique data dirs exist.\n",
    "    base_data_dir = os.path.join(args.data_dir, 'base')\n",
    "    unique_data_dir = os.path.join(args.data_dir, 'unique')\n",
    "    assert os.path.isdir(base_data_dir), f\"No base dir for data at {base_data_dir}.\"\n",
    "    assert os.path.isdir(unique_data_dir), f\"No unique dir for data at {unique_data_dir}.\"\n",
    "\n",
    "    # Set the logger at data_dir\n",
    "    set_logger(os.path.join(args.data_dir, 'build_dataset.log'))\n",
    "\n",
    "    for subdir in os.listdir(base_data_dir):\n",
    "        subdir = os.path.join(base_data_dir, subdir)\n",
    "        expected_data_dir = os.path.join(subdir, 'data_and_target.csv')\n",
    "        build_dataset('patternmining', subdir)\n",
    "\n",
    "    for subdir in os.listdir(unique_data_dir):\n",
    "        subdir = os.path.join(unique_data_dir, subdir)\n",
    "        expected_data_dir = os.path.join(subdir, 'data_and_target.csv')\n",
    "        build_dataset('patternminingV2', subdir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
